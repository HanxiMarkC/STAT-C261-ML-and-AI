---
title: "BIOSTAT C161 HW3"
author: "Hanxi Chen"
subtitle: :)
date: today
format:
  pdf:
    documentclass: article
    number-sections: false
    fig-align: center
    keep-tex: false  # keeps the intermediate .tex file
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

```{r}
library(tidyverse)
library(dplyr)
library(caret)
```

## Q1
### (iii)
Plot the function f over x ∈ [−2.5, 2.5]. How many local/global minima do you see? What
are their approximate values? Can there be other local minima?

**Sol:**

```{r}
# Clear environment
rm(list = ls())


f <- function(x) {
  x^4 - 6*x^2 + 4*x + 18
}

x_vals <- seq(-2.5, 2.5, by = 0.01)
y_vals <- f(x_vals)

plot(x_vals, y_vals, type = "l", col = "blue", lwd = 2,
     main = expression(f(x) == x^4 - 6*x^2 + 4*x + 18),
     xlab = "x", ylab = "f(x)")

grid()
f_prime <- function(x) 4*x^3 - 12*x + 4
roots <- uniroot.all <- function(f, interval, n = 1000, ...) {
  xseq <- seq(interval[1], interval[2], len = n)
  yseq <- f(xseq)
  sign_change <- which(diff(sign(yseq)) != 0)
  sapply(sign_change, function(i)
    uniroot(f, c(xseq[i], xseq[i + 1]), ...)$root)
}

critical_points <- uniroot.all(f_prime, c(-2.5, 2.5))
critical_points

points(critical_points, f(critical_points), col = "red", pch = 19)
text(critical_points, f(critical_points) + 2, 
     labels = round(critical_points, 2), col = "red")

data.frame(
  x = critical_points,
  f_x = f(critical_points)
)

```

The plot of the function over the interval [−2.5, 2.5] shows three stationary points at approximately x = −1.88, 0.35, and 1.53. Based on their function values f(−1.88) ≈ 1.77, f(0.35) ≈ 18.68, and f(1.53) ≈ 15.55, the points at x = −1.88 and x = 1.53 are local minima, while x = 0.35 is a local maximum. The global minimum occurs at x ≈ −1.88 because it gives the smallest f(x). There cannot be any additional local minima since the derivative 4x^3 − 12x + 4 is cubic and thus has at most three real roots.




### (iv)
Write your own code for S steps of GD on this function (do not use built-in or third-party
GD codes).

**Sol:**
```{r}
# Gradient Descent
gradient_descent <- function(x0, alpha, S) {
  x_vals <- numeric(S + 1)  # store all iterates
  x_vals[1] <- x0           # initial value
  grad_vals <- numeric(S + 1)  # store gradients
  
  for (s in 1:S) {
    grad <- f_prime(x_vals[s])
    grad_vals[s] <- grad
    x_vals[s + 1] <- x_vals[s] - alpha * grad
  }
  
  grad_vals[S + 1] <- f_prime(x_vals[S + 1])  # gradient at last step
  

  return(data.frame(Step = 0:S,
                    x = x_vals,
                    f_x = f(x_vals),
                    gradient = grad_vals))
}

```




### (v)
Repeat (i) and (ii) using your code with S = 20 steps. Do you observe convergence in both
cases?

**Sol:**
```{r}
# Case 1: Start from x(0) = 1
result_1 <- gradient_descent(x0 = 1, alpha = 0.1, S = 20)
print(result_1)

# Case 2: Start from x(0) = 0
result_2 <- gradient_descent(x0 = 0, alpha = 0.1, S = 20)
print(result_2)

# Plot convergence paths
plot(result_1$Step, result_1$x, type = "l", col = "blue", lwd = 2,
     ylim = range(c(result_1$x, result_2$x)),
     main = "Convergence Paths of Gradient Descent (α = 0.1, S = 20)",
     xlab = "Step", ylab = "x")
lines(result_2$Step, result_2$x, col = "red", lwd = 2)
legend("topright", legend = c("x(0)=1", "x(0)=0"), col = c("blue", "red"), lwd = 2)

```

The plot shows the convergence behavior of gradient descent with α = 0.1 and S = 20 for two different initial points. When starting from x(0) = 1 (blue line), the algorithm quickly stabilizes near x ≈ 1.53, and the final gradient value of 0.000144 indicates that it has effectively converged to a local minimum. In contrast, when starting from x(0) = 0 (red line), the sequence oscillates strongly between negative and positive values, showing that the updates overshoot due to a large learning rate. The final gradient of −21.396 further confirms divergence rather than convergence. Therefore, gradient descent converges successfully only for the initial value x(0) = 1, while it fails to converge when starting from x(0) = 0 under the same learning rate.




### (vi)
Now set the learning rate to α = 0.01 and repeat (v). Explain why GD performs differently
from (v)

**Sol:**
```{r}
# Case 1: Starting from x(0) = 1
result_x1_small_alpha <- gradient_descent(x0 = 1, alpha = 0.01, S = 20)
print(result_x1_small_alpha)

# Case 2: Starting from x(0) = 0
result_x0_small_alpha <- gradient_descent(x0 = 0, alpha = 0.01, S = 20)
print(result_x0_small_alpha)

plot(result_x1_small_alpha$Step, result_x1_small_alpha$x, 
     type = "b", col = "blue", pch = 19,
     ylim = range(c(result_x1_small_alpha$x, result_x0_small_alpha$x)),
     main = "Convergence Paths of Gradient Descent (α = 0.01, S = 20)",
     xlab = "Step", ylab = "x")
lines(result_x0_small_alpha$Step, result_x0_small_alpha$x, 
      type = "b", col = "red", pch = 17)
legend("bottomright", legend = c("x(0)=1", "x(0)=0"), 
       col = c("blue", "red"), pch = c(19, 17))

```

With a smaller learning rate (α = 0.01), the updates become smoother and more stable. The path starting from x(0) = 1 moves slowly toward the local minimum near x ≈ 1.53, while the path from x(0) = 0 moves gradually toward the global minimum near x ≈ −1.88. However, after 20 steps, neither starting point has reached a point where the gradient is close to 0, indicating that convergence is not yet achieved due to the slower update rate.




## Q2
The College.csv dataset contains admissions data for a sample of 777 universities. We want to
predict the number of applications received (“Apps”) using the other variables in the dataset

### (i)
Let the first 600 observations be the training set and the remaining 177 observations be the
test set


**Sol:**
```{r}
college <- read.csv("./College.csv")

head(college)
college <- college[, -1] 
college$Private <- as.factor(college$Private)
train <- college[1:600, ]
test <- college[601:777, ]

dim(train)
dim(test)

train_x <- model.matrix(Apps ~ ., data = train)[, -1]  # remove intercept
test_x <- model.matrix(Apps ~ ., data = test)[, -1]
train_y <- train$Apps
test_y <- test$Apps
```



### (ii)
fit the OLS regression on the training set, and report the test error obtained.
For the rest of the problem, let the penalization parameter λ vary on the 1000-point grid
from 0.01 to 60 .

**Sol:**
```{r}
library(glmnet)

# Fit OLS model (lambda = 0 no regularization)
ols_fit <- lm(Apps ~ ., data = train)

ols_pred <- predict(ols_fit, newdata = test)

ols_mse <- mean((ols_pred - test_y)^2)
ols_mse
sqrt(ols_mse)
lambda_grid <- seq(0.01, 60, length = 1000)
```

The test MSE = 1502077, RMSE is 1225.593.



### (iii)
Fit the LASSO regression on the training set, with the penalization parameter chosen by
20-fold cross-validation. Report the test error obtained

**Sol:**
```{r}
# alpha = 1 for LASSO
set.seed(123)  
lasso_cv <- cv.glmnet(train_x, train_y, alpha = 1, lambda = lambda_grid, nfolds = 20)

# Best lambda
best_lambda_lasso <- lasso_cv$lambda.min
best_lambda_lasso

lasso_pred <- predict(lasso_cv, s = best_lambda_lasso, newx = test_x)
lasso_mse <- mean((lasso_pred - test_y)^2)
lasso_rmse <- sqrt(lasso_mse)

lasso_mse
lasso_rmse
```
The optimal penalty parameter selected by 20-fold cross-validation is λ = 0.01, yielding a test MSE of approximately 1,499,849 and a corresponding RMSE of about 1224.68, indicating the average prediction error in the number of applications is around 1225.



### (iv)
Fit the ridge regression on the training set, with the penalization parameter chosen by
leave-one-out cross-validation. Report the test error obtained.
**Sol:**
```{r}
set.seed(123)

ridge_cv <- cv.glmnet(train_x, train_y, alpha = 0, lambda = lambda_grid, 
                      nfolds = nrow(train))

# Best lambda
best_lambda_ridge <- ridge_cv$lambda.min
best_lambda_ridge

ridge_pred <- predict(ridge_cv, s = best_lambda_ridge, newx = test_x)
ridge_mse <- mean((ridge_pred - test_y)^2)
ridge_rmse <- sqrt(ridge_mse)

ridge_mse
ridge_rmse
```
The optimal λ selected by leave-one-out cross-validation is 0.01, giving a test MSE of approximately 1,501,329 and an RMSE of about 1225.29. This test error is nearly identical to that of the LASSO model, suggesting that both methods achieve similar predictive performance on this dataset.

### (v)
which of the three models do you prefer? Is there much difference among the test errors?

**Sol:**
All three models: OLS, LASSO, and ridge regression have very similar test errors, with RMSE values all around 1225. This indicates that neither penalization method provides a meaningful improvement over ordinary least squares for predicting the number of applications. Since the predictive performance is nearly identical, the OLS model would be preferred for its simplicity and ease of interpretation, although LASSO could still be useful for variable selection if model sparsity is desired.


