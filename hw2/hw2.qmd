---
title: "BIOSTAT C161 HW2"
author: "Hanxi Chen"
subtitle: :)
date: today
format:
  pdf:
    documentclass: article
    number-sections: false
    fig-align: center
    keep-tex: false  # keeps the intermediate .tex file
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

```{r}
library(tidyverse)
library(dplyr)
library(caret)
```

## Q2
This problem uses the Weekly.csv dataset (uploaded on Bruinlearn) containing 1089 weekly stock
returns for 21 years
### (a)
Use the full dataset to fit a logistic regression of today’s stock movement (up or down) on
the five lags of returns and the trading volume.

**Sol:**
```{r}
# setwd('D:/R/BIOSTAT M236/BIOSTAT_M236_longitudinal/hw2')

Weekly <- read.csv("./Weekly.csv")
head(Weekly)
```

```{r}
Weekly$Direction <- as.factor(Weekly$Direction)

model_full <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                  data = Weekly,
                  family = binomial)

summary(model_full)
```
### (b)
Calculate the confusion matrix, accuracy, precision, recall, and F1 score for the in-sample
predictions. Does the model uniformly beat random guessing in terms of these performance
metrics?

**Sol:**
```{r}
prob_pred <- predict(model_full, type = "response")

pred_class <- ifelse(prob_pred > 0.5, "Up", "Down")
pred_class <- factor(pred_class, levels = levels(Weekly$Direction))

# Confusion matrix
conf_mat <- confusionMatrix(pred_class, Weekly$Direction)
conf_mat

```

```{r}
table_pred <- table(Predicted = pred_class, Actual = Weekly$Direction)
table_pred

# Extract metrics manually
TP <- table_pred["Up", "Up"]
TN <- table_pred["Down", "Down"]
FP <- table_pred["Up", "Down"]
FN <- table_pred["Down", "Up"]

accuracy  <- (TP + TN) / sum(table_pred)
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
F1        <- 2 * precision * recall / (precision + recall)

# Print metrics
cat("Accuracy:", round(accuracy, 3), "\n")
cat("Precision:", round(precision, 3), "\n")
cat("Recall:", round(recall, 3), "\n")
cat("F1 Score:", round(F1, 3), "\n")
```

For random guessing with an equal number of “Up” and “Down” observations, the expected accuracy, precision, recall, and F1 score are all 0.5. In comparison, the fitted logistic regression model yields an accuracy of 0.561, precision of 0.564, recall of 0.921, and F1 score of 0.700. Although the model slightly outperforms random guessing across all metrics, the improvement is modest and largely driven by its strong tendency to predict “Up,” which inflates recall. Therefore, while the model performs better than random guessing numerically, it does not provide substantial predictive power in practice.


### (c)
On the same graph, plot precision and recall against the threshold (varying over [0, 1] ) used
to generate predicted labels from predicted probabilities. Explain the pattern you see

**Sol:**

```{r}
thresholds <- seq(0, 1, by = 0.01)

precision_vals <- numeric(length(thresholds))
recall_vals <- numeric(length(thresholds))

for (i in seq_along(thresholds)) {
  t <- thresholds[i]
  pred_t <- ifelse(prob_pred > t, "Up", "Down")
  pred_t <- factor(pred_t, levels = levels(Weekly$Direction))
  cm <- table(Predicted = pred_t, Actual = Weekly$Direction)
  
  # Extract TP, FP, FN
  TP <- cm["Up", "Up"]
  FP <- cm["Up", "Down"]
  FN <- cm["Down", "Up"]
  
  precision_vals[i] <- TP / (TP + FP)
  recall_vals[i] <- TP / (TP + FN)
}

pr_df <- data.frame(threshold = thresholds,
                    Precision = precision_vals,
                    Recall = recall_vals)

ggplot(pr_df, aes(x = threshold)) +
  geom_line(aes(y = Precision, color = "Precision")) +
  geom_line(aes(y = Recall, color = "Recall")) +
  labs(title = "Precision and Recall vs Threshold",
       x = "Classification Threshold",
       y = "Metric Value") +
  theme_minimal()

```

In this plot, precision and recall exhibit opposite trends as the classification threshold changes. When the threshold is low, almost all observations are predicted as “Up,” so recall remains close to 1 while precision stays around 0.5 because many “Down” cases are incorrectly labeled as “Up.” As the threshold increases, fewer observations are classified as “Up,” causing recall to drop sharply while precision gradually rises, reflecting fewer false positives. The fluctuations in precision at higher thresholds occur because very few positive predictions are made, so small changes in predictions can cause large variations in precision. Overall, the pattern illustrates the fundamental trade-off between precision and recall in binary classification.


### (d)
Now fit the logistic regression using only data up to (and including) the year 2008, with
Lag2 as the only predictor.

**Sol:**
```{r}
train_data <- subset(Weekly, Year <= 2008)

model_lag2 <- glm(Direction ~ Lag2, data = train_data, family = binomial)

summary(model_lag2)
```

### (e)
Repeat (b) using the remaining observations as a test sample.

**Sol:**
```{r}
test_data <- subset(Weekly, Year > 2008)

prob_test <- predict(model_lag2, newdata = test_data, type = "response")

pred_test <- ifelse(prob_test > 0.5, "Up", "Down")

table(Predicted = pred_test, Actual = test_data$Direction)

accuracy <- mean(pred_test == test_data$Direction)

precision <- sum(pred_test == "Up" & test_data$Direction == "Up") /
             sum(pred_test == "Up")

recall <- sum(pred_test == "Up" & test_data$Direction == "Up") /
          sum(test_data$Direction == "Up")

f1 <- 2 * precision * recall / (precision + recall)

cat("Accuracy:", round(accuracy, 3), "\n")
cat("Precision:", round(precision, 3), "\n")
cat("Recall:", round(recall, 3), "\n")
cat("F1 Score:", round(f1, 3), "\n")
```

The model trained using data up to 2008 and tested on later years achieves an accuracy of 0.625, meaning it correctly predicts about 62.5% of weekly market directions. Its precision of 0.622 indicates that when the model predicts “Up,” it is correct about 62% of the time, while the high recall of 0.918 shows that it successfully identifies around 92% of all actual “Up” weeks. The F1 score of 0.742, which balances precision and recall, suggests overall good predictive consistency. Compared to random guessing (which would yield about 0.5 for accuracy, precision, recall, and F1), this model performs notably better, particularly in recall. However, the model tends to overpredict “Up” movements, which explains its high recall but moderate precision.


### (f)
Which of the two fitted models would you use for real-time stock return prediction?

**Sol:**
For real-time stock return prediction, the model trained only on data up to 2008 (the Lag2 model) would be preferred over the model trained on the full dataset. The reason is that this model simulates a true forecasting scenario—using only past information to predict future outcomes—making its performance on post-2008 data a more realistic measure of predictive ability. Although its accuracy (0.625) is modest, it outperforms random guessing and demonstrates generalization beyond the training sample. In contrast, the full-sample model evaluates in-sample performance and thus may suffer from overfitting, offering an overly optimistic view of its predictive power in real-world applications.




